{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "**20 Newsgroups** ( http://qwone.com/~jason/20Newsgroups/ )\n",
    "\n",
    "The dataset has approximately 18,000 Newsgroups documents on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation).\n",
    "\n",
    "For this assignment, I will use only documents from 3 categories ('talk.religion.misc','comp.graphics','sci.space')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pre-processing\n",
    "\n",
    "In this step, we'll download the dataset using the scikit-learn library. The sample program to import the library is given below.\n",
    "\n",
    "This involves two steps:\n",
    "\n",
    "    1) Fetch dataset corresponding to the three following categories:\n",
    "\n",
    "    *  'talk.religion.misc'\n",
    "    *  'comp.graphics'\n",
    "    *  'sci.space'\n",
    "    \n",
    "    2) Remove stop words* and create count vectors for the train and test datasets. \n",
    "    \n",
    "\n",
    "    *Stop words in this context refer to words that occur very frequently and thus contain little information about the type of the article itself.  For e.g., 'a','and','the' etc. See https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/stop_words.py for the list of stop words used in scikit when 'english' is given as input.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from scipy.sparse import find\n",
    "import math as mth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1: ** Fetch the dataset for the three aforementioned categories using scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc','comp.graphics','sci.space']\n",
    "\n",
    "num_categories = len(categories)\n",
    "\n",
    "#Loading training data\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "#Loading testing data\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Loading the class labels for training and testing data\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contatins \n",
      " 1554 train documents, \n",
      " 1034 test documents.\n"
     ]
    }
   ],
   "source": [
    "# Total number of documents in train and test datasets\n",
    "\n",
    "num_train = len(data_train.target)\n",
    "num_test = len(data_test.target)\n",
    "\n",
    "print(\"Dataset contatins \\n \"\n",
    "       +str(num_train)+\" train documents, \\n \"\n",
    "       + str(num_test) + \" test documents.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: nicho@vnet.IBM.COM (Greg Stewart-Nicholls)\n",
      "Subject: Re: Biosphere II\n",
      "Reply-To: nicho@vnet.ibm.com\n",
      "Disclaimer: This posting represents the poster's views, not those of IBM\n",
      "News-Software: UReply 3.1\n",
      "X-X-From: nicho@vnet.ibm.com\n",
      "            <1q1kia$gg8@access.digex.net>\n",
      "Lines: 18\n",
      "\n",
      "In <1q1kia$gg8@access.digex.net> Pat writes:\n",
      ">In article <19930408.043740.516@almaden.ibm.com> nicho@vnet.ibm.com writes:\n",
      ">>In <1q09ud$ji0@access.digex.net> Pat writes:\n",
      ">>>Why is everyone being so critical of B2?\n",
      ">> Because it's bogus science, promoted as 'real' science.\n",
      ">It seems to me, that it's sorta a large engineering project more\n",
      ">then a science project.\n",
      "  Bingo.\n",
      ">B2 is not bench science,  but rather a large scale attempt to\n",
      ">re-create a series of micro-ecologies.   what's so eveil about this?\n",
      " Nothing evil at all. There's no actual harm in what they're doing, only\n",
      "how they represent it.\n",
      "\n",
      " -----------------------------------------------------------------\n",
      " .sig files are like strings ... every yo-yo's got one.\n",
      "\n",
      "Greg Nicholls ... nicho@vnet.ibm.com (business) or\n",
      "                  nicho@olympus.demon.co.uk (private)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "print(data_train.target_names[data_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2:** Remove stop words and create count vectors for the train and test datasets.\n",
    "\n",
    "   We use the CountVectorizer method to extract features (counts for each word). Note that words from both training and testing data are needed to build the count table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(data_train.data + data_test.data)\n",
    "x_train = vectorizer.transform(data_train.data)\n",
    "x_test = vectorizer.transform(data_test.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_array=x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37830"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a Multinomial Naive Bayes classifier that takes feature vector from the test data as input and classifies as one of the three classes ('talk.religion.misc','comp.graphics','sci.space').\n",
    "\n",
    "Complete the training function MultiNB_train() in the cell below to train a Multiomial Naive Bayes classifier that takes \"x_train\",\"y_train\",\"alpha\" as inputs and returns the likelihood probability matrix \"theta\" and the prior distribution  \"prior\" on the document category.\n",
    "\n",
    "\"prior\" is a vector of length equal to num_categories where the $i$-th element is defined as\n",
    "$$ prior (i) = \\frac{\\text{ # of train documents with category i}}{\\text{Total number of train documets}} $$\n",
    "\n",
    "\"theta\" ($\\theta$) is the  matrix with the $(c,i)$th element defined by\n",
    "\n",
    " $$ \\theta(c,i) = P(w_i/c) =  \\frac{N_{ci} + \\alpha }{N_c + |V| \\alpha}$$\n",
    " \n",
    " where,\n",
    " * $P(w_i/c)$ refers to the probability of seeing the $i$th word in the vocabulary given that class type is $c$.\n",
    " * $N_{ci}$ refers to the total number of times the word  $i$ appeared in the training documents of class type $c$.\n",
    " * $N_c$ is the total number of words in the documents of type $c$\n",
    "    $$N_c = \\sum_{d \\in T[c]} N_{cd}$$\n",
    "    where, $T[c]$ refers to the documents of type $c$.\n",
    " * $|V|$ is the size of the vocabulary.\n",
    " * $\\alpha$ is the laplace smoothing parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for calculation total word count in a\n",
    "def calculateWordCount(x_train, individual_wc_count):\n",
    "    word_count = 0\n",
    "    for i in range(len(x_train)):\n",
    "        if(x_train[i]>0):\n",
    "            individual_wc_count[i] +=x_train[i]\n",
    "            word_count += x_train[i]\n",
    "    return (word_count,individual_wc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate prior probabilities of each document\n",
    "def calculatePrior(y_train, num_categories):\n",
    "    doc_cateories_count = [0] * num_categories\n",
    "    doc_total_number = len(y_train)\n",
    "    \n",
    "    for i in range(len(y_train)):\n",
    "        doc_cateories_count[y_train[i]]+=1\n",
    "        \n",
    "    prior = [0] * num_categories\n",
    "    for i in range(num_categories):\n",
    "        prior[i] = float(doc_cateories_count[i])/doc_total_number;\n",
    "    return (prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function for calculating theta\n",
    "def calculateTheta(individual_wc_in_cat, total_wc_in_cat, V, alpha, num_categories):\n",
    "    theta = [[0 for x in range(V)] for y in range(num_categories)]\n",
    "    for categor_number in range(num_categories):\n",
    "        for word_number in range(V):\n",
    "            theta[categor_number][word_number] = float(individual_wc_in_cat[categor_number][word_number]+alpha)/(total_wc_in_cat[categor_number]+(alpha*V))\n",
    "    \n",
    "    return(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MultiNB_train(x_train,y_train, num_categories, alpha):\n",
    "    V = len(vectorizer.get_feature_names()) # Calcualte Size of vocabulary\n",
    "    x_train_array = x_train.toarray() # convert transformed data in two-d array\n",
    "    \n",
    "    total_wc_in_cat = [0] * num_categories\n",
    "    individual_wc_in_cat = [[0 for x in range(V)] for y in range(num_categories)] \n",
    "    prior = calculatePrior(y_train, num_categories)\n",
    "    for i in range(len(x_train_array)):\n",
    "        wc_count,individual_wc_in_cat[y_train[i]] = calculateWordCount(x_train_array[i], individual_wc_in_cat[y_train[i]])\n",
    "        total_wc_in_cat[y_train[i]]+=wc_count\n",
    "    theta = calculateTheta(individual_wc_in_cat, total_wc_in_cat, V, alpha, num_categories)\n",
    "    return(theta, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model to learn the likelihood parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta,prior = MultiNB_train(x_train,y_train, num_categories, alpha = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes Classifier implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MultiNB_classify(x_test_sample, theta, prior,num_categories):\n",
    "    set_prob=True\n",
    "    max_prob = 0\n",
    "    pred_class = 0\n",
    "    x_test_sample_array = x_test_sample.toarray()[0]\n",
    "    for category in range(num_categories):\n",
    "        cat_prob_input = mth.log10(prior[category])\n",
    "        for word in range(len(x_test_sample_array)):\n",
    "            if(x_test_sample_array[word]>0):\n",
    "                cat_prob_input += (mth.log10(theta[category][word]) * x_test_sample_array[word])\n",
    "        if(set_prob == True):\n",
    "            max_prob = cat_prob_input\n",
    "            set_prob = False\n",
    "        if(cat_prob_input > max_prob):\n",
    "            max_prob = cat_prob_input\n",
    "            pred_class = category \n",
    "    return  pred_class\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our classifier on the first sample of testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class:0\n",
      "actual class:0\n"
     ]
    }
   ],
   "source": [
    "pred_class = MultiNB_classify(x_test.getrow(0),theta, prior,num_categories)\n",
    "\n",
    "print(\"predicted class:\" + str(pred_class))\n",
    "print(\"actual class:\" + str(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code below runs your classifier on every data sample from the testing dataset and stored them in \"y_pred\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(num_test):\n",
    "    pred_class = MultiNB_classify(x_test.getrow(i),theta, prior,num_categories)\n",
    "    y_pred.append(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your result by comparing it with the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.959381044487\n",
      "accuracy: 0.959\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.96      0.96       389\n",
      "          1       0.95      0.97      0.96       394\n",
      "          2       0.98      0.94      0.96       251\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "print(score)\n",
    "print(\"accuracy: %0.3f\" % score)\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the classification error (1-score) over the test set for various values of the smoothing parameter α and by trial and error find a good value of α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0099999999999998, 0.95647969052224369)\n",
      "(1.1099999999999999, 0.95551257253384914)\n",
      "(1.0099999999999998, 0.95647969052224369, 0.95551257253384914)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "max_score = -1\n",
    "while 1:\n",
    "    theta,prior = MultiNB_train(x_train,y_train, num_categories, alpha)\n",
    "    y_pred = []\n",
    "    for i in range(num_test):\n",
    "        pred_class = MultiNB_classify(x_test.getrow(i),theta, prior,num_categories)\n",
    "        y_pred.append(pred_class)\n",
    "    score = metrics.accuracy_score(y_test,y_pred)\n",
    "    if(max_score < score):\n",
    "        max_score = score\n",
    "        alpha=alpha+1\n",
    "    elif(max_score == -1):\n",
    "        max_score = score\n",
    "        alpha=alpha+1\n",
    "    else:\n",
    "        break\n",
    "max_score = -1\n",
    "if(alpha < 1):\n",
    "    alpha2 = 0.1\n",
    "elif(alpha<2):\n",
    "    alpha2 = alpha - 1\n",
    "else:\n",
    "    alpha2 = alpha - 2\n",
    "    \n",
    "while alpha2 <= alpha:\n",
    "    theta,prior = MultiNB_train(x_train,y_train, num_categories, alpha2)\n",
    "    y_pred = []\n",
    "    for i in range(num_test):\n",
    "        pred_class = MultiNB_classify(x_test.getrow(i),theta, prior,num_categories)\n",
    "        y_pred.append(pred_class)\n",
    "    score = metrics.accuracy_score(y_test,y_pred)\n",
    "    print(alpha2, score)\n",
    "    if(max_score <= score):\n",
    "        max_score = score\n",
    "        alpha2+=0.1\n",
    "    elif(max_score == -1):\n",
    "        max_score = score\n",
    "        alpha2+=0.1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(alpha2-0.1,max_score,score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
